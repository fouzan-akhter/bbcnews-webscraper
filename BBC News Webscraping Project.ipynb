{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2a0283",
   "metadata": {},
   "source": [
    "# BBC News Webscraping Project\n",
    "\n",
    "Author: Muhammad Fouzan Akhter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca0d92",
   "metadata": {},
   "source": [
    "The code for a web scraping project that targets BBC News is shown below. Underscoring the importance of following website privacy policies is crucial for any online scraping project. It is imperative to highlight that this project is scraping entirely publicly accessible data from Yahoo Finance while adhering to the platform's privacy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing required packages:\n",
    "!pip install beautifulsoup4\n",
    "!pip install selenium\n",
    "!pip install requests\n",
    "!pip install tqdm\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece634d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39adb635",
   "metadata": {},
   "source": [
    "**This Project is coded in the Jupyter Notebook Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca7d11",
   "metadata": {},
   "source": [
    "The following loops extract article data from BBC News. Ensure an active internet connection before executing the code. Set `max_pages` to the maximum number of pages to extract. All the articles on the page will be extracted, including Heading, Date, Author, Content, and Link for each article. All the extracted data is stored in a pandas dataframe. A tqdm loadbar is added for time management while the code is executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "base_url = 'https://www.bbc.com/news/business'\n",
    "driver.get(base_url)\n",
    "max_pages = 50\n",
    "collected_links = set()\n",
    "next_page_selector = \"a.lx-pagination__btn[rel='next']\"\n",
    "current_page = 1\n",
    "while current_page <= max_pages:\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 30).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, next_page_selector))\n",
    "        )\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        links = soup.find_all('a', class_='qa-story-cta-link')\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            full_link = f'bbc.com{href}'\n",
    "            collected_links.add(full_link)\n",
    "        next_button.click()\n",
    "        time.sleep(0)\n",
    "        current_page += 1\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        break\n",
    "driver.quit()\n",
    "num_links_collected = len(collected_links)\n",
    "print(f\"Number of Unique Links Collected: {num_links_collected}\")\n",
    "article_data = []\n",
    "for link in tqdm(collected_links, desc=\"Scraping Articles\"):\n",
    "    article_url = f\"https://{link}\"\n",
    "    response = requests.get(article_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        heading = soup.find('h1', {'tabindex': '-1', 'id': 'main-heading', 'class': 'ssrcss-15xko80-StyledHeading e10rt3ze0'})\n",
    "        heading_text = heading.text if heading else 'Heading not found'\n",
    "        time_element = soup.find('time', {'data-testid': 'timestamp'})\n",
    "        if time_element:\n",
    "            datetime_value = time_element.get('datetime')\n",
    "            dt_obj = datetime.strptime(datetime_value, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            formatted_date = dt_obj.strftime('%b %d, %Y')\n",
    "        else:\n",
    "            formatted_date = 'Date not found'\n",
    "        author_element = soup.find('div', class_='ssrcss-68pt20-Text-TextContributorName e8mq1e96')\n",
    "        author_name = author_element.text.strip() if author_element else 'Author not found'\n",
    "        text_blocks = soup.find_all('div', {'data-component': 'text-block'})\n",
    "        content = \"\\n\".join([text_block.get_text() for text_block in text_blocks])\n",
    "        article_data.append({\n",
    "            \"Heading\": heading_text,\n",
    "            \"Date\": formatted_date,\n",
    "            \"Author\": author_name,\n",
    "            \"Content\": content,\n",
    "            \"Link\": article_url,\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the article at URL: {article_url}\")\n",
    "df = pd.DataFrame(article_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e14c8",
   "metadata": {},
   "source": [
    "**Viewing Scraped Data in Python Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790287dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying scraped data in python environment:\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91727c39",
   "metadata": {},
   "source": [
    "****Exporting Scraped Data as a CSV File****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting scraped data as CSV:\n",
    "df.to_csv('Path on PC/bbc_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ce17b",
   "metadata": {},
   "source": [
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
